{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stopword Removal with NLTK and SpaCy\n",
        "\n",
        "Purpose: Reduces the size of the dataset and focuses on more impactful words.\n",
        "\n",
        "Common Stop Words: \"the\", \"is\", \"at\", \"of\", \"for\", \"to\", \"in\" etc.\n",
        "\n",
        "Tools for Stop Word Removal:\n",
        "- NLTK: ð‘›ð‘™ð‘¡ð‘˜.ð‘ð‘œð‘Ÿð‘ð‘¢ð‘ .ð‘ ð‘¡ð‘œð‘ð‘¤ð‘œð‘Ÿð‘‘ð‘ .\n",
        "- SpaCy: ð‘›ð‘™ð‘.ð·ð‘’ð‘“ð‘Žð‘¢ð‘™ð‘¡ð‘ .ð‘ ð‘¡ð‘œð‘_ð‘¤ð‘œð‘Ÿð‘‘ð‘ ."
      ],
      "metadata": {
        "id": "YfxpabeeRnap"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "V3R4RLL7Rjda"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import string\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "bue2aeO_Sd0z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to remove stopwords using NLTK\n",
        "def remove_stopwords_nltk(text: str) -> str:\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    return \" \".join(filtered_tokens)\n",
        "\n",
        "# Function to remove stopwords using spaCy\n",
        "def remove_stopwords_spacy(text: str) -> str:\n",
        "    doc = nlp(text)\n",
        "    filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
        "    return \" \".join(filtered_tokens)\n"
      ],
      "metadata": {
        "id": "6i4YSYZ-Si3F"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    sample_text = \"This is an example sentence demonstrating the removal of stopwords.\"\n",
        "\n",
        "    # Remove stopwords using NLTK\n",
        "    nltk_filtered_text = remove_stopwords_nltk(sample_text)\n",
        "    print(\"NLTK Stopword Removal:\", nltk_filtered_text)\n",
        "\n",
        "    # Remove stopwords using spaCy\n",
        "    spacy_filtered_text = remove_stopwords_spacy(sample_text)\n",
        "    print(\"spaCy Stopword Removal:\", spacy_filtered_text)"
      ],
      "metadata": {
        "id": "Yd4k4kpiSroc",
        "outputId": "627e88c3-7dd2-4bba-f25c-d46ab48e0bef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Stopword Removal: example sentence demonstrating removal stopwords .\n",
            "spaCy Stopword Removal: example sentence demonstrating removal stopwords .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D-_gL7t5XyRT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}